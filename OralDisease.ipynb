{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing needed modules.\n",
    "from sklearn.metrics import classification_report # used for generating a classification report\n",
    "from os import mkdir , listdir\n",
    "from shutil import copyfile , rmtree #to copy files and remove directories.\n",
    "from random import sample\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers , Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.applications import InceptionV3#InceptionV3 is a popular and powerful pre-trained convolutional neural network (CNN) architecture designed by Google for image classification tasks\n",
    "import matplotlib.pyplot as plt\n",
    "#Instancing from 'ImageDataGenerator' object.\n",
    "generator = ImageDataGenerator(rescale=1/255,\n",
    "                               rotation_range=40,\n",
    "                               width_shift_range=0.2,\n",
    "                               height_shift_range=0.2,\n",
    "                               shear_range=0.2,\n",
    "                               horizontal_flip=True,\n",
    "                               zoom_range=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code defines a function Make_paths that creates a directory structure for training, validation, and testing data\n",
    "#Making train , valid and test directories.\n",
    "def Make_paths(base_path,labels)->str :#Contains the labels or classes for the dataset.\n",
    "    '''function docstring'''\n",
    "    \n",
    "    if 'ready_to_gen' in listdir(base_path) :\n",
    "        rmtree(f'{base_path}/ready_to_gen')\n",
    "    mkdir(f'{base_path}/ready_to_gen')\n",
    "    mkdir(f'{base_path}/ready_to_gen/train')\n",
    "    mkdir(f'{base_path}/ready_to_gen/valid')\n",
    "    mkdir(f'{base_path}/ready_to_gen/test')\n",
    "    for Class in labels :\n",
    "        if Class == 'ready_to_gen' or Class == 'Caries_Gingivitus_ToothDiscoloration_Ulcer-yolo_annotated-Dataset' :\n",
    "            continue\n",
    "            \n",
    "        mkdir(f'{base_path}/ready_to_gen/train/{Class}')\n",
    "        mkdir(f'{base_path}/ready_to_gen/valid/{Class}')\n",
    "        mkdir(f'{base_path}/ready_to_gen/test/{Class}')\n",
    "    return f'{base_path}/ready_to_gen/train' , f'{base_path}/ready_to_gen/valid' , f'{base_path}/ready_to_gen/test'\n",
    "path = \"/kaggle/working/\"\n",
    "train , valid , test =  Make_paths(path,listdir(\"/kaggle/input/oral-diseases\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(photo_path,label_name,train_size)->None :\n",
    "    '''function docstring'''\n",
    "    size_tr = (train_size*len(listdir(photo_path)))//100\n",
    "    train_select = sample(listdir(photo_path),size_tr)\n",
    "    reamin_photos = [photo for photo in listdir(photo_path) if photo not in train_select]\n",
    "    size_valid = len(reamin_photos)//2\n",
    "    valid_select = sample(reamin_photos,size_valid)\n",
    "    for pic in listdir(photo_path) :\n",
    "        if pic in train_select :\n",
    "            copyfile(f'{photo_path}/{pic}',f'{train}/{label_name}/{pic}')\n",
    "        elif pic in valid_select :\n",
    "            copyfile(f'{photo_path}/{pic}',f'{valid}/{label_name}/{pic}')\n",
    "        else :\n",
    "            copyfile(f'{photo_path}/{pic}',f'{test}/{label_name}/{pic}')\n",
    "\n",
    "lbl = [l for l in listdir(\"/kaggle/input/oral-diseases\") if l != 'ready_to_gen' and l != 'Caries_Gingivitus_ToothDiscoloration_Ulcer-yolo_annotated-Dataset']\n",
    "Addresses = [\"/kaggle/input/oral-diseases/Data caries/Data caries/caries augmented data set/preview\",\n",
    "            \"/kaggle/input/oral-diseases/Mouth Ulcer/Mouth Ulcer/Mouth_Ulcer_augmented_DataSet/preview\",\n",
    "            \"/kaggle/input/oral-diseases/Tooth Discoloration/Tooth Discoloration /Tooth_discoloration_augmented_dataser/preview\",\n",
    "            \"/kaggle/input/oral-diseases/hypodontia/hypodontia\",\n",
    "            \"/kaggle/input/oral-diseases/Gingivitis/Gingivitis\",\n",
    "            \"/kaggle/input/oral-diseases/Calculus/Calculus\"]\n",
    "for i in range(len(Addresses)) :\n",
    "    split_data(Addresses[i],lbl[i],80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating data. Data augmentation and generators\n",
    "target_size = (155,155)\n",
    "ready_train = generator.flow_from_directory(train,target_size=target_size,batch_size=20)\n",
    "ready_valid = generator.flow_from_directory(valid,target_size=target_size,batch_size=11)\n",
    "ready_test = generator.flow_from_directory(test,target_size=target_size,batch_size=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = InceptionV3(include_top=False,input_shape=(255,255,3)) #This creates an instance of the InceptionV3 model from Keras' applications module, excluding the top (fully connected) layers\n",
    "for layer in pre.layers :\n",
    "        layer.trainable = False\n",
    "last_l = pre.get_layer('mixed7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_l.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a model by a function.\n",
    "def Create_model(optm,Loss,acc)->None:\n",
    "    '''function docstring'''\n",
    "    pre = InceptionV3(include_top=False,input_shape=(155,155,3))\n",
    "    for layer in pre.layers :\n",
    "        layer.trainable = False\n",
    "    last_l = pre.get_layer('mixed7')\n",
    "    out = last_l.output\n",
    "    x = layers.Flatten()(out)\n",
    "    x = layers.Dense(1000,activation='relu')(x)\n",
    "    x=layers.Normalization()(x)\n",
    "    x = layers.Dense(255,activation='relu')(x)\n",
    "    x=layers.Normalization()(x)\n",
    "    x = layers.Dense(128,activation='relu')(x)\n",
    "    x=layers.Normalization()(x)\n",
    "    x = layers.Dense(64,activation='relu')(x)\n",
    "    x=layers.Normalization()(x)\n",
    "    x = layers.Dense(6,activation='softmax')(x)\n",
    "    model = Model(pre.input,x)\n",
    "    model.compile(optimizer='adam',\n",
    "                 loss=Loss,\n",
    "                 metrics=[acc])\n",
    "    \n",
    "    return model\n",
    "alg = Create_model(RMSprop(learning_rate=0.0001),'categorical_crossentropy','accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alg.load_weights(\"/kaggle/working/weights\")\n",
    "\n",
    "# Assuming 'alg' is your model\n",
    "weights_path = \"/kaggle/working/weights.weights.h5\"  # Change the file extension to .weights.h5\n",
    "alg.save_weights(weights_path)\n",
    "\n",
    "# Load weights\n",
    "try:\n",
    "    alg.load_weights(weights_path)\n",
    "    print(\"Weights loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Error loading weights:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def cosine_learning_rate_schedule(epoch, total_epochs, initial_lr):\n",
    "    \"\"\"\n",
    "    Cosine learning rate schedule.\n",
    "    \n",
    "    Args:\n",
    "        epoch (int): Current epoch number. number in the training process\n",
    "        total_epochs (int): Total number of epochs.\n",
    "        initial_lr (float): Initial learning rate.\n",
    "    \n",
    "    Returns:\n",
    "        lr (float): Updated learning rate for the current epoch.\n",
    "    \"\"\"\n",
    "    max_lr = initial_lr\n",
    "    #Calculates the maximum and minimum learning rates.\n",
    "    min_lr = 0.001 * initial_lr  # You can adjust this minimum LR as needed\n",
    "    \n",
    "    cosine_decay = 0.5 * (1 + np.cos(np.pi * epoch / total_epochs))\n",
    "    lr = min_lr + 0.5 * (max_lr - min_lr) * cosine_decay\n",
    "    \n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler #is a callback in TensorFlow Keras used to dynamically\n",
    "#change the learning rate during training based on a specific function.\n",
    "\n",
    "initial_learning_rate = 0.001  # You can adjust this initial LR as needed\n",
    "total_epochs = 20  # Total number of epochs for training\n",
    "\n",
    "# Create a learning rate scheduler callback\n",
    "lr_scheduler = LearningRateScheduler(lambda epoch: cosine_learning_rate_schedule(epoch, total_epochs, initial_learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # You can choose a different metric like 'val_accuracy'\n",
    "    patience=10,          # Number of epochs with no improvement after which training will be stopped.\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored metric.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code evaluates a model's performance on a test set by computing accuracy,\n",
    "#precision, and recall metrics, providing insight into how well the model performs in classification tasks.\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Assuming you have trained the model and stored the training history in 'history'\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "results = alg.evaluate(ready_test)\n",
    "\n",
    "# Predict the test set\n",
    "predictions = alg.predict(ready_test)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Get true classes from the test set\n",
    "true_classes = ready_test.classes\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = precision_score(true_classes, predicted_classes, average='weighted')\n",
    "recall = recall_score(true_classes, predicted_classes, average='weighted')\n",
    "\n",
    "# Accuracy is available in the 'results' variable from model evaluation\n",
    "accuracy = results[1]  # Assuming accuracy is the second metric\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This setup allows for the evaluation of accuracy, precision, and recall on the validation set after each epoch during model training,\n",
    "#providing insight into the model's performance and convergence\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "class MetricsCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data):\n",
    "        super(MetricsCallback, self).__init__()\n",
    "        self.validation_data = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_pred = self.model.predict(self.validation_data[0])  # Validation images\n",
    "        val_true = np.argmax(self.validation_data[1], axis=1)   # Validation labels\n",
    "        val_pred_classes = np.argmax(val_pred, axis=1)\n",
    "\n",
    "        val_acc = logs['val_accuracy']\n",
    "        val_precision = precision_score(val_true, val_pred_classes, average='weighted')\n",
    "        val_recall = recall_score(val_true, val_pred_classes, average='weighted')\n",
    "\n",
    "        print(f'Epoch: {epoch + 1}, Validation Accuracy: {val_acc:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}')\n",
    "\n",
    "# Assuming you have defined and compiled your model (alg) and have ready_train and ready_valid datasets\n",
    "\n",
    "# Get the validation data as numpy arrays\n",
    "ready_valid_images, ready_valid_labels = next(iter(ready_valid))\n",
    "\n",
    "# Instantiate the custom callback with validation data as numpy arrays\n",
    "metrics_callback = MetricsCallback(validation_data=(ready_valid_images, ready_valid_labels))\n",
    "\n",
    "# Train the model with custom callback\n",
    "history = alg.fit(ready_train, callbacks=[metrics_callback, lr_scheduler], epochs=30, validation_data=ready_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code is used to visualize the performance of a neural network model during training. It plots two graphs\n",
    "#Visualize models performance\n",
    "epoch = range(1,len(history.epoch)+1)\n",
    "results = history.history\n",
    "plt.plot(epoch,results['accuracy'],'blue')\n",
    "plt.plot(epoch,results['val_accuracy'],'red')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Oral_diseases')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epoch,results['loss'],'black')\n",
    "plt.plot(epoch,results['val_loss'],'green')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Oral_diseases')\n",
    "plt.show()\n",
    "\n",
    "#These visualizations help in understanding the trends of accuracy and loss over the training epochs.\n",
    "#It's common to observe how these metrics change over time to evaluate the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating data on test set\n",
    "alg.evaluate(ready_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import requests\n",
    "\n",
    "\n",
    "img_data = requests.get(\"https://assets.nhs.uk/nhsuk-cms/images/S_0118_mouth-ulcer_C0345376.width-1534.jpg\").content\n",
    "with open('image_name.jpg', 'wb') as handler:\n",
    "   handler.write(img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic={num:classes for classes ,num in ready_train.class_indices.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "image_path = r\"/kaggle/input/oral-diseases/Tooth Discoloration/Tooth Discoloration /Tooth_discoloration_augmented_dataser/preview/Tooth_Discoloration_0_1.jpeg\"\n",
    "img = cv2.imread(image_path)#function is used to load an image from the specified path\n",
    "\n",
    "if img is not None:  # Check if image loaded successfully\n",
    "    img = cv2.resize(img, (155, 155))  # Resize the image is used to resize the image to a specified width and height (in this case, to a size of 155x155 pixels).\n",
    "    img = img[..., ::-1] / 255  # Perform normalization or other operations\n",
    "    # Further processing or use of the image\n",
    "else:\n",
    "    print(\"Failed to load the image at:\", image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code snippet checks if the variable i is an instance of either a NumPy array\n",
    "import numpy as np\n",
    "\n",
    "# Example array\n",
    "i = np.array([5, 3, 8, 1, 2])\n",
    "\n",
    "# Check if 'i' is an array or list\n",
    "if isinstance(i, (np.ndarray, list)):\n",
    "    sorted_indices = list(reversed(i.argsort()))\n",
    "    print(\"Sorted Indices:\", sorted_indices)\n",
    "else:\n",
    "    print(\"'i' is not an array or list.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code snippet seems to involve using a trained model (alg) to make predictions on an image represented by the variable img\n",
    "for i in alg.predict(img[None]):\n",
    "        print(dic[i.argmax(-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in alg.predict(img[None]):\n",
    "   for o in list(reversed(i.argsort( ))):\n",
    "        print(dic[o])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir weights2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg.save_weights(\"/kaggle/working/weights.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg.save(\"/kaggle/working/model2.h5\")  # Save model in HDF5 format"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
